version: '3.8'

services:
  # SGLang LLM Services (Brain)
  sglang:
    image: dustynv/sglang:0.4.7-r36.4-cu128-24.04
    container_name: oo1_sglang
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      - ./models/Qwen:/models:ro
      - ./logs/sglang:/workspace/logs
      - sglang_cache:/workspace/cache
    ports:
      - "8001:8001"  # VL-8B vision model
      - "8002:8002"  # 32B brain model
    networks:
      - oo1_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    command: |
      bash -c "
        python3 -m sglang.launch_server \
          --model-path /models/Qwen3-VL-8B-Instruct-FP8 \
          --port 8001 \
          --mem-fraction-static 0.8 \
          --quantization fp8 \
          --log-level info &

        sleep 10

        python3 -m sglang.launch_server \
          --model-path /models/Qwen3-32B-FP8 \
          --port 8002 \
          --mem-fraction-static 0.85 \
          --quantization fp8 \
          --log-level info &

        wait
      "

  # Faster-Whisper ASR Service (Ears)
  faster-whisper:
    image: dustynv/faster-whisper:r36.4.0-cu128-24.04
    container_name: oo1_asr
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - CUDA_VISIBLE_DEVICES=0
      - ASR_MODEL=small
      - COMPUTE_TYPE=int8
      - ASR_LANGUAGE=zh
      - ASR_PORT=8003
    volumes:
      - ./logs/asr:/workspace/logs
      - ./services/asr:/workspace/asr:ro
    ports:
      - "8003:8003"
    networks:
      - oo1_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    working_dir: /workspace
    command: |
      bash -c "
        # Create ASR server from template
        cat > asr_server.py << 'ENDSCRIPT'
import os
import json
import tempfile
from pathlib import Path
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
import uvicorn

try:
    from faster_whisper import WhisperModel
except ImportError:
    print('Installing faster-whisper...')
    os.system('pip install faster-whisper -q')
    from faster_whisper import WhisperModel

app = FastAPI(title='OO1 ASR Service')

MODEL_SIZE = os.getenv('ASR_MODEL', 'small')
COMPUTE_TYPE = os.getenv('COMPUTE_TYPE', 'int8')
LANGUAGE = os.getenv('ASR_LANGUAGE', 'zh')

print(f'Loading Faster-Whisper model: {MODEL_SIZE}')
model = WhisperModel(MODEL_SIZE, device='cuda', compute_type=COMPUTE_TYPE)

@app.get('/v1/models')
async def list_models():
    return {
        'object': 'list',
        'data': [{'id': f'faster-whisper-{MODEL_SIZE}', 'object': 'model', 'owned_by': 'openai-compatible'}]
    }

@app.post('/v1/audio/transcriptions')
async def transcribe(file: UploadFile = File(...), language: str = None):
    try:
        audio_data = await file.read()
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp:
            tmp.write(audio_data)
            tmp_path = tmp.name

        try:
            segments, info = model.transcribe(tmp_path, language=language or LANGUAGE)
            text = ''.join([segment.text for segment in segments])
            return {'text': text, 'language': info.language, 'duration': info.duration}
        finally:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.get('/health')
async def health():
    return {'status': 'ok', 'model': MODEL_SIZE}

if __name__ == '__main__':
    port = int(os.getenv('ASR_PORT', 8003))
    uvicorn.run(app, host='0.0.0.0', port=port, log_level='info')
ENDSCRIPT

        python3 asr_server.py
      "

  # XTTS TTS Service (Mouth)
  xtts:
    image: dustynv/xtts:r36.3.0
    container_name: oo1_tts
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - CUDA_VISIBLE_DEVICES=0
      - TTS_LANGUAGE=zh
      - TTS_PORT=8004
    volumes:
      - ./logs/tts:/workspace/logs
      - ./voices:/workspace/voices
      - ./services/tts:/workspace/tts:ro
    ports:
      - "8004:8004"
    networks:
      - oo1_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8004/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    restart: unless-stopped
    working_dir: /workspace
    command: |
      bash -c "
        # Create TTS server from template
        cat > tts_server.py << 'ENDSCRIPT'
import os
import tempfile
from pathlib import Path
from fastapi import FastAPI, HTTPException
from fastapi.responses import FileResponse
import uvicorn

try:
    from TTS.api import TTS
except ImportError:
    print('Installing TTS library...')
    os.system('pip install TTS -q')
    from TTS.api import TTS

app = FastAPI(title='OO1 TTS Service')

TTS_LANGUAGE = os.getenv('TTS_LANGUAGE', 'zh')
VOICE_DIR = os.getenv('VOICE_DIR', '/workspace/voices')

print(f'Loading XTTS v2 model (language: {TTS_LANGUAGE})...')
tts = TTS(model_name='tts_models/multilingual/multi-dataset/xtts_v2', gpu=True)

Path(VOICE_DIR).mkdir(exist_ok=True, parents=True)

@app.get('/v1/models')
async def list_models():
    return {
        'object': 'list',
        'data': [{'id': 'xtts-v2', 'object': 'model', 'owned_by': 'openai-compatible'}]
    }

@app.get('/v1/voices')
async def list_voices():
    voices = [
        {'id': 'default', 'name': 'Default Female', 'language': TTS_LANGUAGE},
    ]
    for voice_file in Path(VOICE_DIR).glob('*.wav'):
        voices.append({'id': voice_file.stem, 'name': voice_file.stem, 'type': 'cloned'})
    return {'object': 'list', 'data': voices}

@app.post('/v1/audio/speech')
async def synthesize(text: str, voice: str = 'default', language: str = None, speed: float = 1.0):
    try:
        output_file = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
        voice_path = Path(VOICE_DIR) / f'{voice}.wav' if voice != 'default' else None

        if voice_path and voice_path.exists():
            tts.tts_to_file(text=text, file_path=output_file.name, speaker_wav=str(voice_path), language=language or TTS_LANGUAGE, speed=speed)
        else:
            tts.tts_to_file(text=text, file_path=output_file.name, language=language or TTS_LANGUAGE, speed=speed)

        return FileResponse(output_file.name, media_type='audio/wav', filename='speech.wav')
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.get('/health')
async def health():
    return {'status': 'ok', 'model': 'xtts-v2', 'language': TTS_LANGUAGE}

if __name__ == '__main__':
    port = int(os.getenv('TTS_PORT', 8004))
    uvicorn.run(app, host='0.0.0.0', port=port, log_level='info')
ENDSCRIPT

        python3 tts_server.py
      "

networks:
  oo1_network:
    driver: bridge

volumes:
  sglang_cache:
    driver: local
