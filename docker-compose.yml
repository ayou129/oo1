services:
  # SGLang LLM Services (Brain)
  llm:
    image: nvcr.io/nvidia/sglang:25.10-py3
    container_name: llm-q14b
    ipc: host
    network_mode: host
    runtime: nvidia
    privileged: true
    user: "0:0"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
      - MODEL_PATH=Qwen/Qwen3-14B-AWQ
    volumes:
      - ./models:/models
      - ./logs/llm:/workspace/logs
    ports:
      - "8000:8000"
    shm_size: 16gb
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    restart: unless-stopped
    working_dir: /workspace
    command: python -m sglang.launch_server --model-path /models/Qwen/Qwen3-14B-AWQ --served-model-name q14b --port 8000 --trust-remote-code

  # Faster-Whisper ASR Service (Ears)
  faster-whisper:
    image: dustynv/faster-whisper:r36.4.0-cu128-24.04
    container_name: oo1_asr
    ipc: host
    network_mode: host
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - CUDA_VISIBLE_DEVICES=0
      - ASR_MODEL=small
      - ASR_LANGUAGE=zh
      - ASR_PORT=8003
      - COMPUTE_TYPE=int8
    volumes:
      - ./logs/asr:/workspace/logs
      - ./services/asr:/workspace/asr:ro
    ports:
      - "8003:8003"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    restart: unless-stopped
    working_dir: /workspace
    command: python3 /workspace/asr/start_asr.py

  # XTTS TTS Service (Mouth) - Note: CPU-only (single GPU shared with LLM)
  xtts:
    image: dustynv/xtts:r36.3.0
    container_name: oo1_tts
    ipc: host
    network_mode: host
    environment:
      - TTS_LANGUAGE=zh
      - TTS_PORT=8004
      - TTS_DEVICE=cpu
    volumes:
      - ./logs/tts:/workspace/logs
      - ./voices:/workspace/voices
      - ./services/tts:/workspace/tts:ro
    ports:
      - "8004:8004"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8004/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    restart: unless-stopped
    working_dir: /workspace
    command: python3 /workspace/tts/start_tts.py

  # vLLM LLM Services (Brain) - DISABLED TEMPORARILY (FP8 not supported on Thor)
  # llm_vllm:
  #   image: thor_vllm_container:25.08-py3-base
  #   container_name: oo1_llm_vllm
  #   ipc: host
  #   network_mode: host
  #   runtime: nvidia
  #   privileged: true
  #   user: "0:0"
  #   environment:
  #     - NVIDIA_VISIBLE_DEVICES=all
  #     - CUDA_VISIBLE_DEVICES=0
  #     - MODEL_PATH=Qwen/Qwen3-8B-AWQ
  #     - VLLM_DISABLED_KERNELS=MacheteLinearKernel
  #     - VLLM_ATTENTION_BACKEND=FLASH_ATTN
  #   volumes:
  #     - ./models:/models
  #     - ./logs/llm:/workspace/logs
  #     - ./services/llm:/workspace/llm:ro
  #   ports:
  #     - "8000:8000"
  #   shm_size: 16gb
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8000/v1/models"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 120s
  #   restart: unless-stopped
  #   working_dir: /workspace
  #   command: bash /workspace/llm/start_llm.sh

networks:
  oo1_network:
    driver: bridge

volumes:
  llm_cache:
    driver: local
